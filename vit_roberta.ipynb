{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torchtext\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchvision import transforms\n",
    "from transformers import ViTFeatureExtractor, ViTModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed = 59\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chia bộ train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO_val2014_000000396568.jpg extra text\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Chuỗi ban đầu\n",
    "line = \"COCO_val2014_000000396568.jpg#0 extra text\"\n",
    "\n",
    "# Sử dụng regex để xóa mọi thứ sau dấu # và dừng lại khi gặp khoảng trắng\n",
    "line = re.sub(r'#\\S*', '', line)\n",
    "\n",
    "# Loại bỏ khoảng trắng thừa ở cuối (nếu cần)\n",
    "line = line.strip()\n",
    "\n",
    "print(line)  # Kết quả: COCO_val2014_000000396568.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_set_path = './vqa_coco_dataset/vaq2.0.TrainImages.txt'\n",
    "\n",
    "with open(train_set_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line = re.sub(r'#\\S*', '', line)\n",
    "        temp = line.split('\\t')\n",
    "        qa = temp[1].split('?')\n",
    "        answer = qa[1].strip()\n",
    "\n",
    "        data_sample = {\n",
    "            'image_path': temp[0],\n",
    "            'question': qa[0],\n",
    "            'answer': answer\n",
    "        }\n",
    "        train_data.append(data_sample)\n",
    "\n",
    "\n",
    "val_data = []\n",
    "val_set_path = './vqa_coco_dataset/vaq2.0.DevImages.txt'\n",
    "\n",
    "with open(train_set_path, 'r') as f:\n",
    "    for line in f:\n",
    "        temp = line.split('\\t')\n",
    "        qa = temp[1].split('?')\n",
    "        answer = qa[1].strip()\n",
    "\n",
    "        data_sample = {\n",
    "            'image_path': temp[0],\n",
    "            'question': qa[0],\n",
    "            'answer': answer\n",
    "        }\n",
    "        val_data.append(data_sample)\n",
    "\n",
    "\n",
    "test_data = []\n",
    "test_set_path = './vqa_coco_dataset/vaq2.0.TestImages.txt'\n",
    "\n",
    "with open(train_set_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        temp = line.split('\\t')\n",
    "        qa = temp[1].split('?')\n",
    "        answer = qa[1].strip()\n",
    "\n",
    "        data_sample = {\n",
    "            'image_path': temp[0],\n",
    "            'question': qa[0] + '?',\n",
    "            'answer': answer\n",
    "        }\n",
    "        test_data.append(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Asus\\Ungdung\\Miniconda\\workspace\\envs\\AIOEx\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = './vqa_coco_dataset/val2014-resised/COCO_val2014_000000000133.jpg'\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\n",
    "    'google/vit-base-patch16-224-in21k')\n",
    "\n",
    "inputs = feature_extractor(\n",
    "    images=img, return_tensors=\"pt\")    \n",
    "inputs = inputs['pixel_values'].squeeze().permute(1, 2, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# # Original image\n",
    "# axes[0].imshow(img)\n",
    "# axes[0].axis('off')\n",
    "# axes[0].set_title('Original Image')\n",
    "\n",
    "# # Features image\n",
    "# axes[1].imshow(inputs)\n",
    "# axes[1].axis('off')\n",
    "# axes[1].set_title('Feature Image')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We need to remove 7 to truncate the input but the first sequence has a length 7. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([    0, 31414,   232,  1368,   298,  4001,  4001,   118,     2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "tokenizer(\"Hello world hhihihi\", max_length=20, padding=\"max_length\",\n",
    "          truncation=True, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, max_seq_length):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "    encode_text = tokenizer(text, max_length=20, padding=\"max_length\",\n",
    "                            truncation=True, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
    "\n",
    "    if len(encode_text) > max_seq_length:\n",
    "        encode_text = encode_text[:max_seq_length]\n",
    "\n",
    "    return encode_text\n",
    "\n",
    "def feature_extractor(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(\n",
    "        'google/vit-base-patch16-224-in21k')\n",
    "    \n",
    "    inputs = feature_extractor(images=img, return_tensors=\"pt\")\n",
    "    inputs = inputs['pixel_values'].squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng mapping labels dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set(\n",
    "    sample['answer'] for sample in train_data\n",
    ")\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng Pytorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADatasets(Dataset):\n",
    "    def __init__(self, data, label2idx, max_seq_length=20, transform=None, tokenize=None, feature_extractor = None,  img_dir='./vqa_coco_dataset/val2014-resised'):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.label2idx = label2idx\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.transform = transform\n",
    "        self.tokenize = tokenize\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            img_path = os.path.join(\n",
    "                self.img_dir, self.data[index]['image_path'])\n",
    "            img = self.feature_extractor(img_path)\n",
    "            \n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "\n",
    "            questions = self.data[index]['question']\n",
    "            questions = torch.tensor(self.tokenize(\n",
    "                questions, self.max_seq_length))\n",
    "\n",
    "            answer = self.data[index]['answer']\n",
    "            id_label = self.label2idx[answer]\n",
    "            id_label = torch.tensor(id_label)\n",
    "\n",
    "            return img, questions, id_label\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {index}: {e}\")\n",
    "            print(f\"Data at index {index}: {self.data[index]}\")\n",
    "            raise  # Tùy chọn: bạn có thể giữ hoặc bỏ dòng này nếu muốn chương trình dừng khi gặp lỗi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(size=(224, 224)),\n",
    "        transforms.CenterCrop(size=180),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.GaussianBlur(3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(size=(224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIOEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
