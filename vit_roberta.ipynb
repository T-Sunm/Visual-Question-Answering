{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torchtext\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchvision import transforms\n",
    "from transformers import ViTFeatureExtractor, ViTModel, RobertaTokenizer, RobertaModel  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed = 59\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chia bộ train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO_val2014_000000396568.jpg extra text\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Chuỗi ban đầu\n",
    "line = \"COCO_val2014_000000396568.jpg#0 extra text\"\n",
    "\n",
    "# Sử dụng regex để xóa mọi thứ sau dấu # và dừng lại khi gặp khoảng trắng\n",
    "line = re.sub(r'#\\S*', '', line)\n",
    "\n",
    "# Loại bỏ khoảng trắng thừa ở cuối (nếu cần)\n",
    "line = line.strip()\n",
    "\n",
    "print(line)  # Kết quả: COCO_val2014_000000396568.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_set_path = './vqa_coco_dataset/vaq2.0.TrainImages.txt'\n",
    "\n",
    "with open(train_set_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = re.sub(r'#\\S*', '', line)\n",
    "        temp = line.split('\\t')\n",
    "        qa = temp[1].split('?')\n",
    "\n",
    "        if len(qa) == 2:\n",
    "            answer = qa[1].strip()\n",
    "        elif len(qa) == 3:\n",
    "            answer = qa[2].strip()\n",
    "\n",
    "        data_sample = {\n",
    "            'image_path': temp[0],\n",
    "            'question': qa[0] + '?',\n",
    "            'answer': answer\n",
    "        }\n",
    "        train_data.append(data_sample)\n",
    "\n",
    "\n",
    "val_data = []\n",
    "val_set_path = './vqa_coco_dataset/vaq2.0.DevImages.txt'\n",
    "\n",
    "with open(train_set_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = re.sub(r'#\\S*', '', line)\n",
    "        temp = line.split('\\t')\n",
    "        qa = temp[1].split('?')\n",
    "\n",
    "        if len(qa) == 2:\n",
    "            answer = qa[1].strip()\n",
    "        elif len(qa) == 3:\n",
    "            answer = qa[2].strip()\n",
    "\n",
    "        data_sample = {\n",
    "            'image_path': temp[0],\n",
    "            'question': qa[0] + '?',\n",
    "            'answer': answer\n",
    "        }\n",
    "        val_data.append(data_sample)\n",
    "\n",
    "\n",
    "test_data = []\n",
    "test_set_path = './vqa_coco_dataset/vaq2.0.TestImages.txt'\n",
    "\n",
    "with open(train_set_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = re.sub(r'#\\S*', '', line)\n",
    "        temp = line.split('\\t')\n",
    "        qa = temp[1].split('?')\n",
    "\n",
    "        if len(qa) == 2:\n",
    "            answer = qa[1].strip()\n",
    "        elif len(qa) == 3:\n",
    "            answer = qa[2].strip()\n",
    "\n",
    "        data_sample = {\n",
    "            'image_path': temp[0],\n",
    "            'question': qa[0] + '?',\n",
    "            'answer': answer\n",
    "        }\n",
    "        test_data.append(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, max_seq_length, device):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "    encode_text = tokenizer(\n",
    "        text,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded = {key: value.squeeze().to(device)\n",
    "               for key, value in encode_text.items()}\n",
    "    if len(encode_text['input_ids']) > max_seq_length:\n",
    "        encoded = {key: value[:max_seq_length]\n",
    "                   for key, value in encode_text.items()}\n",
    "    return encoded\n",
    "\n",
    "def feature_extractor(img, device):\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(\n",
    "        'google/vit-base-patch16-224-in21k'\n",
    "    )\n",
    "    inputs = feature_extractor(images=img, return_tensors=\"pt\")\n",
    "    encoded = {key: value.squeeze().to(device)\n",
    "               for key, value in inputs.items()}\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized output shape: torch.Size([20])\n",
      "Extracted feature shape: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "## Test hàm tokenize và feature_extractor\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Thiết bị\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Test hàm tokenize\n",
    "sample_text = \"This is a sample text.\"\n",
    "max_seq_length = 20\n",
    "encoded_text = tokenize(sample_text, max_seq_length, device)\n",
    "print(\"Tokenized output shape:\", encoded_text[\"input_ids\"].shape)\n",
    "\n",
    "# Test hàm feature_extractor\n",
    "sample_img_path = \"./vqa_coco_dataset/val2014-resised/COCO_val2014_000000000133.jpg\"\n",
    "img = Image.open(sample_img_path).convert('RGB')\n",
    "features = feature_extractor(img, device)\n",
    "print(\"Extracted feature shape:\", features[\"pixel_values\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng mapping labels dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 0, 'no': 1}\n"
     ]
    }
   ],
   "source": [
    "labels = set(\n",
    "    sample['answer'] for sample in train_data\n",
    ")\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng Pytorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADatasets(Dataset):\n",
    "    def __init__(self, data, label2idx, max_seq_length=20, transform=None, tokenize=None, feature_extractor=None, device='cpu', img_dir='./vqa_coco_dataset/val2014-resised'):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.label2idx = label2idx\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.transform = transform\n",
    "        self.tokenize = tokenize\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.img_dir = img_dir\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Lấy đường dẫn ảnh\n",
    "        img_path = os.path.join(self.img_dir, self.data[index]['image_path'])\n",
    "\n",
    "        # Mở ảnh và chuyển sang RGB\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Áp dụng transform nếu có\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Trích xuất đặc trưng ảnh\n",
    "        img = self.feature_extractor(img, self.device)\n",
    "\n",
    "        # Xử lý câu hỏi\n",
    "        questions = self.data[index]['question']\n",
    "        questions = self.tokenize(questions, self.max_seq_length, self.device)\n",
    "\n",
    "        # Xử lý nhãn\n",
    "        answer = self.data[index]['answer']\n",
    "        id_label = self.label2idx[answer]\n",
    "        id_label = torch.tensor(id_label).to(self.device)\n",
    "\n",
    "        # Tạo sample\n",
    "        sample = {\n",
    "            'image': img,\n",
    "            'question': questions,\n",
    "            'label': id_label\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(size=(224, 224)),\n",
    "        transforms.CenterCrop(size=180),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.GaussianBlur(3),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(size=(224, 224)),\n",
    "        transforms.CenterCrop(size=180),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.GaussianBlur(3),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Khai báo datasets object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_dataset = VQADatasets(\n",
    "    train_data,\n",
    "    label2idx=label2id,\n",
    "    max_seq_length=20, transform=data_transform['train'], tokenize=tokenize, feature_extractor=feature_extractor, device=device\n",
    ")\n",
    "val_dataset = VQADatasets(\n",
    "    val_data,\n",
    "    label2idx=label2id,\n",
    "    max_seq_length=20, transform=data_transform['val'], tokenize=tokenize, feature_extractor=feature_extractor, device=device\n",
    ")\n",
    "test_dataset = VQADatasets(\n",
    "    test_data,\n",
    "    label2idx=label2id,\n",
    "    max_seq_length=20, transform=data_transform['val'], tokenize=tokenize, feature_extractor=feature_extractor, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "Image shape: {'pixel_values': tensor([[[ 0.8745,  0.8667,  0.8588,  ...,  0.1294, -0.0353, -0.0980],\n",
      "         [ 0.8667,  0.8588,  0.8510,  ...,  0.2784,  0.0588, -0.0353],\n",
      "         [ 0.8588,  0.8510,  0.8431,  ...,  0.4667,  0.2157,  0.0275],\n",
      "         ...,\n",
      "         [ 0.0039,  0.0275,  0.0118,  ..., -0.4745, -0.5059, -0.4824],\n",
      "         [-0.0745, -0.0353, -0.0118,  ..., -0.3490, -0.2863, -0.2706],\n",
      "         [-0.1059, -0.1294, -0.0980,  ..., -0.0980, -0.1059, -0.1608]],\n",
      "\n",
      "        [[ 0.9451,  0.9373,  0.9294,  ...,  0.1686,  0.0510,  0.0196],\n",
      "         [ 0.9373,  0.9294,  0.9216,  ...,  0.3098,  0.1294,  0.0667],\n",
      "         [ 0.9294,  0.9216,  0.9137,  ...,  0.4902,  0.2784,  0.1216],\n",
      "         ...,\n",
      "         [-0.0824, -0.0431, -0.0275,  ..., -0.6471, -0.6627, -0.6157],\n",
      "         [-0.1686, -0.1137, -0.0745,  ..., -0.5373, -0.4667, -0.4196],\n",
      "         [-0.2157, -0.2235, -0.1765,  ..., -0.2863, -0.2863, -0.3255]],\n",
      "\n",
      "        [[ 0.9373,  0.9294,  0.9216,  ...,  0.1059,  0.0275,  0.0196],\n",
      "         [ 0.9294,  0.9216,  0.9137,  ...,  0.2392,  0.0824,  0.0510],\n",
      "         [ 0.9373,  0.9294,  0.9137,  ...,  0.4118,  0.2000,  0.0510],\n",
      "         ...,\n",
      "         [-0.3098, -0.2471, -0.2078,  ..., -0.8431, -0.8667, -0.8196],\n",
      "         [-0.4118, -0.3333, -0.2706,  ..., -0.7333, -0.6784, -0.6549],\n",
      "         [-0.4588, -0.4510, -0.3804,  ..., -0.5137, -0.5216, -0.5843]]])}\n",
      "Question: {'input_ids': tensor([    0,  6209,    42,    10, 28110, 14532, 17487,     2,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "Label: tensor(1)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Duyệt qua train_dataset và in thử một vài phần tử\n",
    "for idx, sample in enumerate(train_dataset):\n",
    "    print(f\"Sample {idx}:\")\n",
    "    print(\"Image shape:\", sample['image'])  # Kiểm tra hình dạng của ảnh\n",
    "    print(\"Question:\", sample['question'])        # Kiểm tra câu hỏi\n",
    "    print(\"Label:\", sample['label'])              # Kiểm tra nhãn\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 256\n",
    "test_batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Asus\\Ungdung\\Miniconda\\workspace\\envs\\AIOEx\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch:\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(train_loader))\n",
    "print(\"Sample batch:\")\n",
    "print(sample_batch['label'].size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "total += sample_batch['label'].size(0)\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = RobertaModel.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VisualEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VissionEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassfiyModel(nn.Module):\n",
    "    def __init__(self, hidden_size = 512,dropout_prob = 0.2 ,num_classes = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(768*2, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VQA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, visual_encoder, text_encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.visual_encoder = visual_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, image, answer):\n",
    "        text_out = self.text_encoder(answer)\n",
    "        image_out = self.visual_encoder(image)\n",
    "\n",
    "        x = torch.cat((image_out, text_out), dim =1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def freeze(self, visual=True, textual=True, classifier=True):\n",
    "        if visual:\n",
    "            for n, p in self.visual_encoder.named_parameters():\n",
    "                p.requires_grad = visual\n",
    "\n",
    "        if textual:\n",
    "            for n, p in self.text_encoder.named_parameters():\n",
    "                p.requires_grad = textual\n",
    "\n",
    "        if classifier:\n",
    "            for param in self.classifier.parameters():\n",
    "                param.requires_grad = classifier\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(labels)\n",
    "hidden_size = 512\n",
    "dropout_prob = 0.2\n",
    "\n",
    "text_encoder = TextEncoder().to(device)\n",
    "visual_encoder = VissionEncoder().to(device)\n",
    "\n",
    "classifier = ClassfiyModel(hidden_size=hidden_size, dropout_prob=dropout_prob, num_classes=n_classes).to(device)\n",
    "\n",
    "model = VQAModel(visual_encoder, text_encoder, classifier).to(device)\n",
    "model.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "epochs = 50\n",
    "scheduler_step_size = epochs * 0.8\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=scheduler_step_size,\n",
    "    gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataLoader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "\n",
    "    for i, sample in enumerate(dataLoader):\n",
    "        img = sample[\"image\"]\n",
    "        question = sample[\"question\"]\n",
    "        label = sample[\"label\"]\n",
    "\n",
    "        outputs = model(img, question)\n",
    "        loss = criterion(outputs, label)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        total += label.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == label).sum().item()\n",
    "        break\n",
    "    loss = sum(losses) / len(dataLoader)\n",
    "    acc = correct / total\n",
    "\n",
    "    return loss , acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    epochs\n",
    "):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_train_losses = []\n",
    "\n",
    "        model.train()\n",
    "        for idx, inputs in enumerate(train_loader):\n",
    "            images = inputs['image']\n",
    "            questions = inputs['question']\n",
    "            labels = inputs['label']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, questions)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_train_losses.append(loss.item())\n",
    "\n",
    "        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        val_loss, val_acc = evaluate(\n",
    "            model, val_loader,\n",
    "            criterion\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(\n",
    "            f'EPOCH {epoch + 1}: Train loss: {train_loss:.4f} Val loss: {val_loss:.4f} Val Acc: {val_acc}')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    epochs\n",
    "):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_train_losses = []\n",
    "\n",
    "        model.train()\n",
    "        for idx, inputs in enumerate(train_loader):\n",
    "            images = inputs['image']\n",
    "            questions = inputs['question']\n",
    "            labels = inputs['label']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, questions)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_train_losses.append(loss.item())\n",
    "\n",
    "        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        val_loss, val_acc = evaluate(\n",
    "            model, val_loader,\n",
    "            criterion\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(\n",
    "            f'EPOCH {epoch + 1}: Train loss: {train_loss:.4f} Val loss: {val_loss:.4f} Val Acc: {val_acc}')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIOEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
